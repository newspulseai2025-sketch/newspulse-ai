{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a270fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. IMPORT LIBRARIES\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# 1. IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scaling + metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# LSTM (TensorFlow)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Technical indicators\n",
    "import ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280797ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DEFINE PROJECT PATHS\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\moham\\Downloads\\scrape\\text\\crypto_project\")\n",
    "\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "OUTPUT_FIGS = PROJECT_ROOT / \"outputs\" / \"figures\"\n",
    "\n",
    "# Create folders if not existing\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "OUTPUT_FIGS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ETH_FILE = DATA_PROCESSED / \"eth_cleaned.csv\"\n",
    "NEWS_FILE = DATA_PROCESSED / \"daily_news_aggregated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BASE PATH + FILE NAMES\n",
    "from pathlib import Path\n",
    "base_path = Path(r\"C:\\Users\\moham\\Downloads\\scrape\\text\\crypto_project\\data\\processed\")\n",
    "eth_file = base_path / \"eth_cleaned.csv\"\n",
    "news_file = base_path / \"daily_news_aggregated.csv\"\n",
    "\n",
    "\n",
    "# 2. LOAD DATASETS\n",
    "eth_df = pd.read_csv(eth_file)\n",
    "daily_news = pd.read_csv(news_file)\n",
    "\n",
    "\n",
    "# 3. CLEAN + PARSE DATES\n",
    "eth_df[\"date\"] = pd.to_datetime(eth_df[\"date\"], errors=\"coerce\")\n",
    "daily_news[\"date\"] = pd.to_datetime(daily_news[\"date\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# 4. DEFINE DATE RANGE\n",
    "start_date = pd.to_datetime(\"2023-01-12\")\n",
    "end_date   = pd.to_datetime(\"2025-10-16\")\n",
    "\n",
    "eth_df_filtered = eth_df[(eth_df[\"date\"] >= start_date) & (eth_df[\"date\"] <= end_date)].copy()\n",
    "daily_news_filtered = daily_news[(daily_news[\"date\"] >= start_date) & (daily_news[\"date\"] <= end_date)].copy()\n",
    "\n",
    "eth_df_filtered = eth_df_filtered.sort_values(\"date\").reset_index(drop=True)\n",
    "daily_news_filtered = daily_news_filtered.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 5. MERGE ETH + NEWS\n",
    "eth_news_merged = pd.merge(\n",
    "    eth_df_filtered,\n",
    "    daily_news_filtered,\n",
    "    on=\"date\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "eth_news_merged[\"news_count\"] = eth_news_merged[\"news_count\"].fillna(0)\n",
    "eth_news_merged[\"mean_sentiment\"] = eth_news_merged[\"mean_sentiment\"].fillna(0)\n",
    "\n",
    "\n",
    "# 6. COMPUTE TECHNICAL INDICATORS\n",
    "import ta\n",
    "\n",
    "eth_news_merged[\"rsi\"] = ta.momentum.RSIIndicator(close=eth_news_merged[\"eth_close\"], window=14).rsi()\n",
    "\n",
    "macd = ta.trend.MACD(close=eth_news_merged[\"eth_close\"])\n",
    "eth_news_merged[\"macd\"] = macd.macd()\n",
    "eth_news_merged[\"macd_signal\"] = macd.macd_signal()\n",
    "eth_news_merged[\"macd_diff\"] = macd.macd_diff()\n",
    "\n",
    "eth_news_merged[\"sma_20\"] = eth_news_merged[\"eth_close\"].rolling(window=20).mean()\n",
    "eth_news_merged[\"sma_50\"] = eth_news_merged[\"eth_close\"].rolling(window=50).mean()\n",
    "\n",
    "bb = ta.volatility.BollingerBands(close=eth_news_merged[\"eth_close\"], window=20, window_dev=2)\n",
    "eth_news_merged[\"bb_upper\"] = bb.bollinger_hband()\n",
    "eth_news_merged[\"bb_lower\"] = bb.bollinger_lband()\n",
    "eth_news_merged[\"bb_middle\"] = bb.bollinger_mavg()\n",
    "\n",
    "\n",
    "# 7. TARGET VARIABLE\n",
    "eth_news_merged[\"target_next_close\"] = eth_news_merged[\"eth_close\"].shift(-1)\n",
    "eth_news_merged = eth_news_merged.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 8. PREPARE FEATURES + SCALING\n",
    "features = [\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"macd_diff\",\n",
    "    \"sma_20\", \"sma_50\", \"bb_upper\", \"bb_lower\", \"bb_middle\",\n",
    "    \"news_count\", \"mean_sentiment\"\n",
    "]\n",
    "\n",
    "X = eth_news_merged[features].values\n",
    "y = eth_news_merged[\"target_next_close\"].values.reshape(-1,1)\n",
    "\n",
    "# Scale features\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "\n",
    "# 9. CREATE LSTM SEQUENCE DATA\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i+time_steps)])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 10\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "\n",
    "# Train/test split (80/20)\n",
    "train_size = int(len(X_seq)*0.8)\n",
    "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "\n",
    "# 10. BUILD LSTM MODEL\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "# 11. TRAIN MODEL\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 12. PREDICTION\n",
    "\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# 13. EVALUATE\n",
    "mse = mean_squared_error(y_test_actual, y_pred)\n",
    "rmse = mse**0.5\n",
    "r2 = r2_score(y_test_actual, y_pred)\n",
    "\n",
    "print(f\"LSTM Results: RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "# 14. PLOT ACTUAL VS PREDICTED\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(y_test_actual, label=\"Actual Price\", color=\"blue\")\n",
    "plt.plot(y_pred, label=\"Predicted Price\", color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"LSTM ETH Price Prediction (RMSE={rmse:.2f})\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"ETH Closing Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 15. PREDICT NEXT N DAYS\n",
    "n_days = 14\n",
    "last_sequence = X_scaled[-time_steps:].copy()  # last sequence for prediction\n",
    "pred_future_scaled = []\n",
    "\n",
    "current_seq = last_sequence.copy()\n",
    "\n",
    "for _ in range(n_days):\n",
    "    input_seq = current_seq.reshape(1, time_steps, X_scaled.shape[1])\n",
    "    next_pred_scaled = model.predict(input_seq)\n",
    "    pred_future_scaled.append(next_pred_scaled[0,0])\n",
    "    \n",
    "    next_row = current_seq[-1].copy() \n",
    "    current_seq = np.vstack((current_seq[1:], next_row.reshape(1,-1)))\n",
    "\n",
    "pred_future = scaler_y.inverse_transform(np.array(pred_future_scaled).reshape(-1,1))\n",
    "\n",
    "# Create future dates\n",
    "last_date = eth_news_merged[\"date\"].iloc[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_days)\n",
    "\n",
    "future_df = pd.DataFrame({\n",
    "    \"date\": future_dates,\n",
    "    \"eth_close\": np.nan,\n",
    "    \"rsi\": np.nan,\n",
    "    \"macd\": np.nan,\n",
    "    \"macd_signal\": np.nan,\n",
    "    \"macd_diff\": np.nan,\n",
    "    \"sma_20\": np.nan,\n",
    "    \"sma_50\": np.nan,\n",
    "    \"bb_upper\": np.nan,\n",
    "    \"bb_lower\": np.nan,\n",
    "    \"bb_middle\": np.nan,\n",
    "    \"news_count\": np.nan,\n",
    "    \"mean_sentiment\": np.nan,\n",
    "    \"target_next_close\": pred_future.flatten()\n",
    "})\n",
    "\n",
    "combined_df = pd.concat([eth_news_merged, future_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# 16. PLOT HISTORICAL + PREDICTIONS\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(combined_df[\"date\"], combined_df[\"eth_close\"], label=\"Historical ETH Close\", color=\"blue\")\n",
    "plt.plot(combined_df[\"date\"], combined_df[\"target_next_close\"], label=\"Predicted ETH Close\", color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"ETH Historical + {n_days}-Day Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ETH Closing Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import ta\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# 1. BASE PATH + FILE NAMES\n",
    "base_path = Path(r\"C:\\Users\\moham\\Downloads\\scrape\\text\\crypto_project\\data\")\n",
    "eth_file = base_path / \"eth_cleaned.csv\"\n",
    "news_file = base_path / \"daily_news_aggregated.csv\"\n",
    "\n",
    "\n",
    "# 2. LOAD DATASETS\n",
    "eth_df = pd.read_csv(eth_file)\n",
    "daily_news = pd.read_csv(news_file)\n",
    "\n",
    "\n",
    "# 3. CLEAN + PARSE DATES\n",
    "eth_df[\"date\"] = pd.to_datetime(eth_df[\"date\"], errors=\"coerce\")\n",
    "daily_news[\"date\"] = pd.to_datetime(daily_news[\"date\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# 4. FILTER DATE RANGE\n",
    "start_date = pd.to_datetime(\"2023-01-12\")\n",
    "end_date = pd.to_datetime(\"2025-10-16\")\n",
    "\n",
    "eth_df = eth_df[(eth_df[\"date\"] >= start_date) & (eth_df[\"date\"] <= end_date)].copy()\n",
    "daily_news = daily_news[(daily_news[\"date\"] >= start_date) & (daily_news[\"date\"] <= end_date)].copy()\n",
    "\n",
    "eth_df = eth_df.sort_values(\"date\").reset_index(drop=True)\n",
    "daily_news = daily_news.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 5. MERGE ETH + NEWS\n",
    "eth_news_merged = pd.merge(eth_df, daily_news, on=\"date\", how=\"left\")\n",
    "eth_news_merged[\"news_count\"] = eth_news_merged[\"news_count\"].fillna(0)\n",
    "eth_news_merged[\"mean_sentiment\"] = eth_news_merged[\"mean_sentiment\"].fillna(0)\n",
    "\n",
    "\n",
    "# 6. TECHNICAL INDICATORS\n",
    "eth_news_merged[\"rsi\"] = ta.momentum.RSIIndicator(close=eth_news_merged[\"eth_close\"], window=14).rsi()\n",
    "macd = ta.trend.MACD(close=eth_news_merged[\"eth_close\"])\n",
    "eth_news_merged[\"macd\"] = macd.macd()\n",
    "eth_news_merged[\"macd_signal\"] = macd.macd_signal()\n",
    "eth_news_merged[\"macd_diff\"] = macd.macd_diff()\n",
    "eth_news_merged[\"sma_20\"] = eth_news_merged[\"eth_close\"].rolling(window=20).mean()\n",
    "eth_news_merged[\"sma_50\"] = eth_news_merged[\"eth_close\"].rolling(window=50).mean()\n",
    "bb = ta.volatility.BollingerBands(close=eth_news_merged[\"eth_close\"], window=20, window_dev=2)\n",
    "eth_news_merged[\"bb_upper\"] = bb.bollinger_hband()\n",
    "eth_news_merged[\"bb_lower\"] = bb.bollinger_lband()\n",
    "eth_news_merged[\"bb_middle\"] = bb.bollinger_mavg()\n",
    "\n",
    "\n",
    "# 7. TARGET VARIABLE\n",
    "eth_news_merged[\"target_next_close\"] = eth_news_merged[\"eth_close\"].shift(-1)\n",
    "eth_news_merged = eth_news_merged.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 8. FEATURES + SCALING\n",
    "features = [\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"macd_diff\",\n",
    "    \"sma_20\", \"sma_50\", \"bb_upper\", \"bb_lower\", \"bb_middle\",\n",
    "    \"news_count\", \"mean_sentiment\"\n",
    "]\n",
    "\n",
    "X = eth_news_merged[features].values\n",
    "y = eth_news_merged[\"target_next_close\"].values.reshape(-1,1)\n",
    "\n",
    "# Separate scalers for X and y\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "\n",
    "# 9. CREATE SEQUENCES\n",
    "def create_sequences(X, y, time_steps=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 30\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "\n",
    "# Train/test split\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "\n",
    "# 10. BUILD STACKED BIDIRECTIONAL LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# 11. TRAIN WITH EARLY STOPPING\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# 12. PREDICTION\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "# 13. EVALUATE\n",
    "mse = mean_squared_error(y_test_actual, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_actual, y_pred)\n",
    "print(f\"Stacked BiLSTM Results: RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "\n",
    "# 14. PLOT ACTUAL VS PREDICTED\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(y_test_actual, label=\"Actual Price\", color=\"blue\")\n",
    "plt.plot(y_pred, label=\"Predicted Price\", color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"Stacked BiLSTM ETH Prediction (RMSE={rmse:.2f})\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"ETH Closing Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 15. PREDICT NEXT N DAYS\n",
    "n_days = 14\n",
    "last_sequence = X_scaled[-time_steps:].copy()\n",
    "pred_future_scaled = []\n",
    "current_seq = last_sequence.copy()\n",
    "\n",
    "for _ in range(n_days):\n",
    "    input_seq = current_seq.reshape(1, time_steps, X_scaled.shape[1])\n",
    "    next_pred_scaled = model.predict(input_seq)\n",
    "    pred_future_scaled.append(next_pred_scaled[0,0])\n",
    "    \n",
    "    # Shift sequence and repeat last features\n",
    "    next_row = current_seq[-1].copy()\n",
    "    current_seq = np.vstack((current_seq[1:], next_row.reshape(1,-1)))\n",
    "\n",
    "pred_future = scaler_y.inverse_transform(np.array(pred_future_scaled).reshape(-1,1))\n",
    "\n",
    "\n",
    "# 16. FUTURE DATAFRAME\n",
    "last_date = eth_news_merged[\"date\"].iloc[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_days)\n",
    "\n",
    "future_df = pd.DataFrame({\n",
    "    \"date\": future_dates,\n",
    "    \"eth_close\": np.nan,\n",
    "    \"rsi\": np.nan,\n",
    "    \"macd\": np.nan,\n",
    "    \"macd_signal\": np.nan,\n",
    "    \"macd_diff\": np.nan,\n",
    "    \"sma_20\": np.nan,\n",
    "    \"sma_50\": np.nan,\n",
    "    \"bb_upper\": np.nan,\n",
    "    \"bb_lower\": np.nan,\n",
    "    \"bb_middle\": np.nan,\n",
    "    \"news_count\": np.nan,\n",
    "    \"mean_sentiment\": np.nan,\n",
    "    \"target_next_close\": pred_future.flatten()\n",
    "})\n",
    "\n",
    "combined_df = pd.concat([eth_news_merged, future_df], ignore_index=True)\n",
    "\n",
    "# 17. PLOT HISTORICAL + FUTURE\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(combined_df[\"date\"], combined_df[\"eth_close\"], label=\"Historical ETH Close\", color=\"blue\")\n",
    "plt.plot(combined_df[\"date\"], combined_df[\"target_next_close\"], label=\"Predicted ETH Close\", color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"ETH Historical + {n_days}-Day Forecast (Stacked BiLSTM)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ETH Closing Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302919a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import ta\n",
    "\n",
    "\n",
    "# 1. LOAD DATA\n",
    "base_path = Path(r\"C:\\Users\\moham\\Downloads\\scrape\\text\\crypto_project\\data\")\n",
    "eth_file = base_path / \"eth_cleaned.csv\"\n",
    "news_file = base_path / \"daily_news_aggregated.csv\"\n",
    "\n",
    "eth_df = pd.read_csv(eth_file)\n",
    "daily_news = pd.read_csv(news_file)\n",
    "\n",
    "eth_df[\"date\"] = pd.to_datetime(eth_df[\"date\"], errors=\"coerce\")\n",
    "daily_news[\"date\"] = pd.to_datetime(daily_news[\"date\"], errors=\"coerce\")\n",
    "\n",
    "start_date = pd.to_datetime(\"2023-01-12\")\n",
    "end_date = pd.to_datetime(\"2025-10-16\")\n",
    "\n",
    "eth_df = eth_df[(eth_df[\"date\"] >= start_date) & (eth_df[\"date\"] <= end_date)].copy()\n",
    "daily_news = daily_news[(daily_news[\"date\"] >= start_date) & (daily_news[\"date\"] <= end_date)].copy()\n",
    "\n",
    "eth_df = eth_df.sort_values(\"date\").reset_index(drop=True)\n",
    "daily_news = daily_news.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Merge\n",
    "df = pd.merge(eth_df, daily_news, on=\"date\", how=\"left\")\n",
    "df[\"news_count\"] = df[\"news_count\"].fillna(0)\n",
    "df[\"mean_sentiment\"] = df[\"mean_sentiment\"].fillna(0)\n",
    "\n",
    "\n",
    "# 2. TECHNICAL INDICATORS\n",
    "df[\"rsi\"] = ta.momentum.RSIIndicator(close=df[\"eth_close\"], window=14).rsi()\n",
    "macd = ta.trend.MACD(close=df[\"eth_close\"])\n",
    "df[\"macd\"] = macd.macd()\n",
    "df[\"macd_signal\"] = macd.macd_signal()\n",
    "df[\"macd_diff\"] = macd.macd_diff()\n",
    "df[\"sma_20\"] = df[\"eth_close\"].rolling(20).mean()\n",
    "df[\"sma_50\"] = df[\"eth_close\"].rolling(50).mean()\n",
    "bb = ta.volatility.BollingerBands(close=df[\"eth_close\"], window=20, window_dev=2)\n",
    "df[\"bb_upper\"] = bb.bollinger_hband()\n",
    "df[\"bb_lower\"] = bb.bollinger_lband()\n",
    "df[\"bb_middle\"] = bb.bollinger_mavg()\n",
    "\n",
    "df[\"target_next_close\"] = df[\"eth_close\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 3. FEATURES + SCALING\n",
    "features = [\"rsi\",\"macd\",\"macd_signal\",\"macd_diff\",\"sma_20\",\"sma_50\",\n",
    "            \"bb_upper\",\"bb_lower\",\"bb_middle\",\"news_count\",\"mean_sentiment\"]\n",
    "\n",
    "X = df[features].values\n",
    "y = df[\"target_next_close\"].values.reshape(-1,1)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "\n",
    "# 4. FUNCTION TO CREATE SEQUENCES\n",
    "def create_sequences(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "# 5. HYPERPARAMETER OPTIONS\n",
    "# time_steps_list = [10, 20, 30]\n",
    "# lstm_units_list = [32, 64, 128]\n",
    "# dropout_list = [0.1, 0.2, 0.3]\n",
    "# batch_size_list = [16, 32]\n",
    "# epochs_list = [50, 100]\n",
    "# learning_rate_list = [0.001, 0.0005]\n",
    "\n",
    "#test parameters\n",
    "time_steps_list = [10]\n",
    "lstm_units_list = [32]\n",
    "dropout_list = [0.1]\n",
    "batch_size_list = [8]\n",
    "epochs_list = [50]\n",
    "learning_rate_list = [0.01]\n",
    "\n",
    "\n",
    "# 6. LOOP THROUGH HYPERPARAMETERS\n",
    "results = []\n",
    "\n",
    "for time_steps in time_steps_list:\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "    train_size = int(len(X_seq)*0.8)\n",
    "    X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "    y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "    \n",
    "    for lstm_units in lstm_units_list:\n",
    "        for dropout_rate in dropout_list:\n",
    "            for batch_size in batch_size_list:\n",
    "                for epochs in epochs_list:\n",
    "                    for lr in learning_rate_list:\n",
    "                        # Build model\n",
    "                        model = Sequential()\n",
    "                        model.add(LSTM(lstm_units, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                        model.add(Dense(1))\n",
    "                        model.compile(optimizer=Adam(learning_rate=lr), loss=\"mse\")\n",
    "                        \n",
    "                        # Train model\n",
    "                        history = model.fit(\n",
    "                            X_train, y_train,\n",
    "                            validation_split=0.1,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose=0,\n",
    "                            shuffle=False\n",
    "                        )\n",
    "                        \n",
    "                        # Predict\n",
    "                        y_pred_scaled = model.predict(X_test)\n",
    "                        y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "                        y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred))\n",
    "                        r2 = r2_score(y_test_actual, y_pred)\n",
    "                        \n",
    "                        # Store results\n",
    "                        results.append({\n",
    "                            \"time_steps\": time_steps,\n",
    "                            \"lstm_units\": lstm_units,\n",
    "                            \"dropout\": dropout_rate,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"RMSE\": rmse,\n",
    "                            \"R2\": r2\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"Done: time_steps={time_steps}, units={lstm_units}, dropout={dropout_rate}, batch={batch_size}, epochs={epochs}, lr={lr} | RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "                        # Update best model\n",
    "                        if rmse < best_rmse:\n",
    "                            best_rmse = rmse\n",
    "                            best_model = model\n",
    "                            best_X_test = X_test\n",
    "                            best_y_test = y_test_actual\n",
    "                            best_y_pred = y_pred\n",
    "\n",
    "\n",
    "# 7. RESULTS DATAFRAME\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(\"RMSE\").head(10))\n",
    "\n",
    "\n",
    "\n",
    "# 8. SAVE BEST MODEL\n",
    "best_model.save(\"best_lstm_model.h5\")\n",
    "print(\"Best model saved as best_lstm_model.h5\")\n",
    "\n",
    "\n",
    "# 9. PLOT BEST MODEL PREDICTION\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(best_y_test, label=\"Actual Price\", color=\"blue\")\n",
    "plt.plot(best_y_pred, label=\"Predicted Price\", color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"Best LSTM Model Prediction (RMSE={best_rmse:.2f})\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"ETH Closing Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ca546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import ta\n",
    "\n",
    "\n",
    "# 1. LOAD DATA\n",
    "base_path = Path(r\"C:\\Users\\moham\\Downloads\\scrape\\text\\crypto_project\\data\")\n",
    "eth_file = base_path / \"eth_cleaned.csv\"\n",
    "news_file = base_path / \"daily_news_aggregated.csv\"\n",
    "\n",
    "eth_df = pd.read_csv(eth_file)\n",
    "daily_news = pd.read_csv(news_file)\n",
    "\n",
    "eth_df[\"date\"] = pd.to_datetime(eth_df[\"date\"], errors=\"coerce\")\n",
    "daily_news[\"date\"] = pd.to_datetime(daily_news[\"date\"], errors=\"coerce\")\n",
    "\n",
    "start_date = pd.to_datetime(\"2023-01-12\")\n",
    "end_date = pd.to_datetime(\"2025-10-16\")\n",
    "\n",
    "eth_df = eth_df[(eth_df[\"date\"] >= start_date) & (eth_df[\"date\"] <= end_date)].copy()\n",
    "daily_news = daily_news[(daily_news[\"date\"] >= start_date) & (daily_news[\"date\"] <= end_date)].copy()\n",
    "\n",
    "eth_df = eth_df.sort_values(\"date\").reset_index(drop=True)\n",
    "daily_news = daily_news.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Merge\n",
    "df = pd.merge(eth_df, daily_news, on=\"date\", how=\"left\")\n",
    "df[\"news_count\"] = df[\"news_count\"].fillna(0)\n",
    "df[\"mean_sentiment\"] = df[\"mean_sentiment\"].fillna(0)\n",
    "\n",
    "\n",
    "# 2. TECHNICAL INDICATORS\n",
    "df[\"rsi\"] = ta.momentum.RSIIndicator(close=df[\"eth_close\"], window=14).rsi()\n",
    "macd = ta.trend.MACD(close=df[\"eth_close\"])\n",
    "df[\"macd\"] = macd.macd()\n",
    "df[\"macd_signal\"] = macd.macd_signal()\n",
    "df[\"macd_diff\"] = macd.macd_diff()\n",
    "df[\"sma_20\"] = df[\"eth_close\"].rolling(20).mean()\n",
    "df[\"sma_50\"] = df[\"eth_close\"].rolling(50).mean()\n",
    "bb = ta.volatility.BollingerBands(close=df[\"eth_close\"], window=20, window_dev=2)\n",
    "df[\"bb_upper\"] = bb.bollinger_hband()\n",
    "df[\"bb_lower\"] = bb.bollinger_lband()\n",
    "df[\"bb_middle\"] = bb.bollinger_mavg()\n",
    "\n",
    "df[\"target_next_close\"] = df[\"eth_close\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 3. FEATURES + SCALING\n",
    "features = [\"rsi\",\"macd\",\"macd_signal\",\"macd_diff\",\"sma_20\",\"sma_50\",\n",
    "            \"bb_upper\",\"bb_lower\",\"bb_middle\",\"news_count\",\"mean_sentiment\"]\n",
    "\n",
    "X = df[features].values\n",
    "y = df[\"target_next_close\"].values.reshape(-1,1)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# 4. FUNCTION TO CREATE SEQUENCES\n",
    "def create_sequences(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# 5. HYPERPARAMETER OPTIONS\n",
    "# time_steps_list = [10, 20, 30]\n",
    "# lstm_units_list = [32, 64, 128]\n",
    "# dropout_list = [0.1, 0.2, 0.3]\n",
    "# batch_size_list = [16, 32]\n",
    "# epochs_list = [50, 100]\n",
    "# learning_rate_list = [0.001, 0.0005]\n",
    "\n",
    "#test parameters\n",
    "time_steps_list = [10]\n",
    "lstm_units_list = [32]\n",
    "dropout_list = [0.1]\n",
    "batch_size_list = [8]\n",
    "epochs_list = [50]\n",
    "learning_rate_list = [0.01]\n",
    "\n",
    "\n",
    "# 6. LOOP THROUGH HYPERPARAMETERS\n",
    "results = []\n",
    "best_rmse = np.inf\n",
    "best_model = None\n",
    "best_X_test, best_y_test = None, None\n",
    "best_y_pred = None\n",
    "\n",
    "for time_steps in time_steps_list:\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, time_steps)\n",
    "    train_size = int(len(X_seq)*0.8)\n",
    "    X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "    y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "    \n",
    "    for lstm_units in lstm_units_list:\n",
    "        for dropout_rate in dropout_list:\n",
    "            for batch_size in batch_size_list:\n",
    "                for epochs in epochs_list:\n",
    "                    for lr in learning_rate_list:\n",
    "                        # Build model\n",
    "                        model = Sequential()\n",
    "                        model.add(LSTM(lstm_units, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                        model.add(Dense(1))\n",
    "                        model.compile(optimizer=Adam(learning_rate=lr), loss=\"mse\")\n",
    "                        \n",
    "                        # Train model\n",
    "                        history = model.fit(\n",
    "                            X_train, y_train,\n",
    "                            validation_split=0.1,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose=0,\n",
    "                            shuffle=False\n",
    "                        )\n",
    "                        \n",
    "                        # Predict\n",
    "                        y_pred_scaled = model.predict(X_test)\n",
    "                        y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "                        y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred))\n",
    "                        r2 = r2_score(y_test_actual, y_pred)\n",
    "                        \n",
    "                        # Store results\n",
    "                        results.append({\n",
    "                            \"time_steps\": time_steps,\n",
    "                            \"lstm_units\": lstm_units,\n",
    "                            \"dropout\": dropout_rate,\n",
    "                            \"batch_size\": batch_size,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"RMSE\": rmse,\n",
    "                            \"R2\": r2\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"Done: time_steps={time_steps}, units={lstm_units}, dropout={dropout_rate}, batch={batch_size}, epochs={epochs}, lr={lr} | RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
    "                        \n",
    "                        # Update best model\n",
    "                        if rmse < best_rmse:\n",
    "                            best_rmse = rmse\n",
    "                            best_model = model\n",
    "                            best_X_test = X_test\n",
    "                            best_y_test = y_test_actual\n",
    "                            best_y_pred = y_pred\n",
    "\n",
    "# 7. RESULTS DATAFRAME\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(\"RMSE\").head(10))\n",
    "\n",
    "\n",
    "# 8. SAVE BEST MODEL\n",
    "best_model.save(\"best_lstm_model.h5\")\n",
    "print(\"Best model saved as best_lstm_model.h5\")\n",
    "\n",
    "\n",
    "# 9. PLOT BEST MODEL PREDICTION\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(best_y_test, label=\"Actual Price\", color=\"blue\")\n",
    "plt.plot(best_y_pred, label=\"Predicted Price\", color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"Best LSTM Model Prediction (RMSE={best_rmse:.2f})\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"ETH Closing Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
